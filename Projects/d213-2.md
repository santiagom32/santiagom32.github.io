---
layout: default
title: "Neural Network-Based Sentiment Analysis on Amazon Product Reviews"
---

# Neural Network-Based Sentiment Analysis on Amazon Product Reviews

## Comprehensive Project Overview

This project is an in-depth exploration into the use of neural networks, specifically Long Short-Term Memory (LSTM) models, combined with Natural Language Processing (NLP) techniques for sentiment analysis. Utilizing the "amazon_cells_labelled.txt" dataset, the project aims to predict the sentiment of product reviews, distinguishing between positive and negative sentiments.

## Objective and Goals

- **Primary Objective:** To develop a neural network model that normalizes and interprets text data from product reviews for accurate sentiment prediction.
- **Secondary Goal:** Creating a machine learning model that understands context in text sequences and accurately categorizes reviews as either positive or negative.

## Research Question

"Can a neural network model, trained on the 'amazon_cells_labelled.txt' dataset, effectively predict the sentiment of new, unseen product reviews?"

## Network Choice

**Network Type:** Long Short-Term Memory (LSTM), an ideal choice for processing and understanding the context in sequential text data like product reviews.

## Data Preparation and Analysis

**Exploratory Data Analysis:**
- Unusual Characters: Lowercasing and removing characters using regex to ensure clean text data.

<img width="450" alt="image" src="https://github.com/santiagom32/santiagom32.github.io/assets/138883598/fe6bac32-666a-477c-9484-4225abf1f7de">

<img width="450" alt="image" src="https://github.com/santiagom32/santiagom32.github.io/assets/138883598/cd8cdbce-d180-4b69-ae02-6a1cb457a2d0">


- Vocabulary Size: Original 2237 unique values reduced to 1867 after stop word removal. A dictionary size of 2000 is set to encapsulate the entire word dictionary, enhancing model accuracy.

<img width="450" alt="image" src="https://github.com/santiagom32/santiagom32.github.io/assets/138883598/f0ec2abc-bc80-4346-873c-1145de1118a2">

- Word Embedding Length: Optimal length set at 50, balancing detailed representation and model efficiency.

- Maximum Sequence Length: Set at 16, based on descriptive statistics of the reviews, ensuring coverage of most data while minimizing noise.
  
**Tokenization and Vectorization Goals:**
- Process: Utilizing nltk and Keras Tokenizer for converting text into a format suitable for the neural network.

<img width="450" alt="image" src="https://github.com/santiagom32/santiagom32.github.io/assets/138883598/1abcbe22-c990-4008-8c3a-ca9c745c3d5b">

- Objective: Transforming text into individual words and then into numeric vectors for model processing.
  
**Padding Process:**
- Standardization: Padding post-text sequence to ensure consistent length of input data.

  <img width="450" alt="image" src="https://github.com/santiagom32/santiagom32.github.io/assets/138883598/17cb1211-9c37-4b66-b328-7b3d426f83dd">

- Visualization: Example of a single padded sequence provided.
  
**Sentiment Categories and Activation Function:**
- Categories: Binary sentiment classification (0 for negative, 1 for positive).
- Activation Function: Sigmoid function in the output layer for binary output classification.
  
**Data Preparation Steps:**
- Process: Tokenization, vectorization, and data splitting into 80% training and 20% testing sets, with 20% of training data for validation.
- Dataset Access: Prepared dataset available in attached files.

## Network Architecture

**Model Summary Output:**
- Layers: 4-layer network including Embedding, Dropout, LSTM, and Dense layers.
- Parameters: Total of 414,675 trainable parameters.

<img width="450" alt="image" src="https://github.com/santiagom32/santiagom32.github.io/assets/138883598/d0bbb5ba-b449-479a-ba12-d3dd3c734f6e">

  
**Hyperparameter Justification:**
- Activation Functions: Sigmoid function for binary classification.
- Nodes per Layer: 256 nodes in the LSTM layer to capture complex language patterns.
- Loss Function: Binary cross-entropy for measuring model accuracy in binary classification.
- Optimizer: Adam optimizer for adaptive learning rates.
- Stopping Criteria: Early Stopping based on a delta of 0.001 and a patience setting of 10.
- Evaluation Metric: Accuracy as the primary evaluation metric.

## Model Evaluation

**Impact of Stopping Criteria:**
- Efficiency: Early stopping determines the optimal number of epochs, enhancing model efficiency and efficacy.
- Final Training Epoch: Screenshot showing the final training epoch included.

<img width="450" alt="image" src="https://github.com/santiagom32/santiagom32.github.io/assets/138883598/063ed22b-b536-42f7-bd2b-2838e38b1936">

  
**Model Fitness and Overfitting:**
- Performance: Good predictive accuracy with a loss of 0.9975 and accuracy of 0.7900.
- Overfitting Mitigation: Early stopping and dropout layers implemented to prevent overfitting.
  
**Training Process Visualization:**
- Graphs: Line graphs of loss and accuracy metrics during the training process.

<img width="450" alt="image" src="https://github.com/santiagom32/santiagom32.github.io/assets/138883598/1cbfd8d6-12f3-4e06-b8b6-a14cb5e3445d">

<img width="450" alt="image" src="https://github.com/santiagom32/santiagom32.github.io/assets/138883598/b4d90389-d1af-47b6-ab2f-bc0a3adaaef4">

  
**Predictive Accuracy:**
- Accuracy Metric: Model achieves a predictive accuracy of 79%, indicating substantial predictive capability.

## Summary and Recommendations

**Functionality and Network Architecture Impact:**
- Model Functionality: Demonstrates a predictive accuracy of 79%, attributed to its multi-layer architecture.
- Network Architecture Components: Embedding layer for dense vector transformation, LSTM layer for sequential data processing, and dropout layer for overfitting prevention.

**Recommended Course of Action:**
- Application: The model can be deployed to classify new reviews with 79% accuracy. Itâ€™s especially useful for processing large volumes of reviews and extracting sentiment insights.
- Future Enhancements: Further fine-tuning and incorporating additional layers or advanced NLP techniques could enhance the model's accuracy and applicability.

