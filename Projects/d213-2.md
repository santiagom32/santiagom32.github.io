Detailed Project Structure Format 1: Neural Network-Based Sentiment Analysis on Amazon Product Reviews
Comprehensive Project Overview

This project is an in-depth exploration into the use of neural networks, specifically Long Short-Term Memory (LSTM) models, combined with Natural Language Processing (NLP) techniques for sentiment analysis. Utilizing the "amazon_cells_labelled.txt" dataset, the project aims to predict the sentiment of product reviews, distinguishing between positive and negative sentiments.

Objective and Goals

Primary Objective: To develop a neural network model that normalizes and interprets text data from product reviews for accurate sentiment prediction.
Secondary Goal: Creating a machine learning model that understands context in text sequences and accurately categorizes reviews as either positive or negative.
Research Question

"Can a neural network model, trained on the 'amazon_cells_labelled.txt' dataset, effectively predict the sentiment of new, unseen product reviews?"

Network Choice

Network Type: Long Short-Term Memory (LSTM), an ideal choice for processing and understanding the context in sequential text data like product reviews.
Data Preparation and Analysis

Exploratory Data Analysis:
Unusual Characters: Lowercasing and removing characters using regex to ensure clean text data.
Vocabulary Size: Original 2237 unique values reduced to 1867 after stop word removal. A dictionary size of 2000 is set to encapsulate the entire word dictionary, enhancing model accuracy.
Word Embedding Length: Optimal length set at 50, balancing detailed representation and model efficiency.
Maximum Sequence Length: Set at 16, based on descriptive statistics of the reviews, ensuring coverage of most data while minimizing noise.
Tokenization and Vectorization Goals:
Process: Utilizing nltk and Keras Tokenizer for converting text into a format suitable for the neural network.
Objective: Transforming text into individual words and then into numeric vectors for model processing.
Padding Process:
Standardization: Padding post-text sequence to ensure consistent length of input data.
Visualization: Example of a single padded sequence provided.
Sentiment Categories and Activation Function:
Categories: Binary sentiment classification (0 for negative, 1 for positive).
Activation Function: Sigmoid function in the output layer for binary output classification.
Data Preparation Steps:
Process: Tokenization, vectorization, and data splitting into 80% training and 20% testing sets, with 20% of training data for validation.
Dataset Access: Prepared dataset available in attached files.
Network Architecture

Model Summary Output:
Layers: 4-layer network including Embedding, Dropout, LSTM, and Dense layers.
Parameters: Total of 414,675 trainable parameters.
Hyperparameter Justification:
Activation Functions: Sigmoid function for binary classification.
Nodes per Layer: 256 nodes in the LSTM layer to capture complex language patterns.
Loss Function: Binary cross-entropy for measuring model accuracy in binary classification.
Optimizer: Adam optimizer for adaptive learning rates.
Stopping Criteria: Early Stopping based on a delta of 0.001 and a patience setting of 10.
Evaluation Metric: Accuracy as the primary evaluation metric.
Model Evaluation

Impact of Stopping Criteria:
Efficiency: Early stopping determines the optimal number of epochs, enhancing model efficiency and efficacy.
Final Training Epoch: Screenshot showing the final training epoch included.
Model Fitness and Overfitting:
Performance: Good predictive accuracy with a loss of 0.9975 and accuracy of 0.7900.
Overfitting Mitigation: Early stopping and dropout layers implemented to prevent overfitting.
Training Process Visualization:
Graphs: Line graphs of loss and accuracy metrics during the training process.
Predictive Accuracy:
Accuracy Metric: Model achieves a predictive accuracy of 79%, indicating substantial predictive capability.
Summary and Recommendations

Functionality and Network Architecture Impact:
Model Functionality: Demonstrates a predictive accuracy of 79%, attributed to its multi-layer architecture.
Network Architecture Components: Embedding layer for dense vector transformation, LSTM layer for sequential data processing, and dropout layer for overfitting prevention.
Recommended Course of Action:
Application: The model can be deployed to classify new reviews with 79% accuracy. Itâ€™s especially useful for processing large volumes of reviews and extracting sentiment insights.
Future Enhancements: Further fine-tuning and incorporating additional layers or advanced NLP techniques could enhance the model's accuracy and applicability.
Code for Saving the Trained Network:
Model Save Function: model.save('d213-2_sentiment.h5')
